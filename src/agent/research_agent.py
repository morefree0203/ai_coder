import json
from typing import List, Dict, Any

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
# æ–¹æ³•1ï¼šä½¿ç”¨ langchain-openai åŒ…ï¼ˆæŽ¨èï¼Œå…¼å®¹ 1.x æœ€æ–°è§„èŒƒï¼‰
from langchain_openai import ChatOpenAI
# æ–¹æ³•2ï¼šå¦‚æžœåªè£…äº† openai åŒ…ï¼Œç”¨æ—§è·¯å¾„å…¼å®¹ï¼ˆ1.0.8 ä»æ”¯æŒï¼‰
# from langchain.chat_models.openai import ChatOpenAI

# æ¶ˆæ¯ç±»çš„å¯¼å…¥è·¯å¾„ä¿®æ­£ï¼ˆ1.x ç‰ˆæœ¬ç»Ÿä¸€åœ¨ langchain.schema ä¸‹ï¼‰
from .prompt import (
    SYSTEM_RESEARCH_BASE,
    PLAN_PROMPT,
    SYNTHESIS_PROMPT,
    CRITIQUE_PROMPT
)
from .memory import ConversationMemory
from .tools import MCPToolClient, WebSearchTool
from ..config.settings import settings

def build_chat_llm_from_agent(agent_key: str) -> ChatOpenAI:
    """
    é€šè¿‡ agents.yaml çš„æŒ‡å®š agent é…ç½®æž„å»º ChatOpenAIã€‚
    ModelScope æä¾› OpenAI å…¼å®¹çš„ APIï¼ˆbase_url + api_keyï¼‰ã€‚
    """
    cfg = settings.get_agent_config(agent_key)
    model = cfg.get("model")
    api_key = cfg.get("api_key")
    base_url = cfg.get("base_url", None)
    temperature = cfg.get("temperature", 0.3)
    max_tokens = cfg.get("max_tokens", 2048)

    # ChatOpenAI æ”¯æŒè‡ªå®šä¹‰ base_url ä¸Ž api_keyï¼ˆOpenAIå…¼å®¹æŽ¥å£ï¼‰
    llm = ChatOpenAI(
        model=model,
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return llm

class ResearchAgent:
    """
    å•ç”¨æˆ·å¤šè½® Research Agentã€‚
    ä½¿ç”¨ agents.yaml çš„ research é…ç½®æ¥åˆå§‹åŒ–æ¨¡åž‹ã€‚
    """
    def __init__(self, agent_key: str = "research"):
        # ä¸» LLM
        self.llm = build_chat_llm_from_agent(agent_key)

        # è®°å¿†ç®¡ç†å™¨
        self.memory = ConversationMemory()

        # åŽ‹ç¼©è®°å¿† LLMï¼ˆå¯å¤ç”¨ä¸»æ¨¡åž‹ï¼Œæˆ–ç”¨é…ç½®é‡Œ memory_summary_modelï¼‰
        cfg = settings.get_agent_config(agent_key)
        summary_model = cfg.get("memory_summary_model", cfg.get("model"))
        self.memory.summarizer_llm = ChatOpenAI(
            model=summary_model,
            api_key=cfg.get("api_key"),
            base_url=cfg.get("base_url"),
            temperature=0.2,
            max_tokens=1024
        )

        # å·¥å…·åˆå§‹åŒ–
        self.tools = {}
        if settings.enable_search_tool:
            mcp_client = MCPToolClient(settings.mcp_config_path)
            self.tools["web_search"] = WebSearchTool(mcp_client)

        self.system_message = SystemMessage(content=SYSTEM_RESEARCH_BASE)

    def _plan(self, query: str) -> List[Dict[str, str]]:
        plan_prompt = PLAN_PROMPT.format(
            query=query,
            max_subquestions=settings.max_subquestions if hasattr(settings, "max_subquestions") else 5
        )
        response = self.llm.invoke([self.system_message, HumanMessage(content=plan_prompt)])
        text = response.content.strip()
        try:
            data = json.loads(text)
            if isinstance(data, list):
                max_n = getattr(settings, "max_subquestions", 5)
                return data[: max_n]
        except Exception:
            return [{"subq": query, "reason": "åŽŸå§‹é—®é¢˜ï¼ˆJSONè§£æžå¤±è´¥å›žé€€ï¼‰"}]
        return [{"subq": query, "reason": "åŽŸå§‹é—®é¢˜ï¼ˆæœªè¯†åˆ«ç»“æž„ï¼‰"}]

    def _search(self, subquestions: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        search_tool = self.tools.get("web_search")
        if not search_tool:
            print("ðŸ” æœªè¿›è¡Œ Web Researchï¼šæœç´¢å·¥å…·æœªå¯ç”¨")
            return [{"subq": sq["subq"], "results": [], "error": "æœç´¢å·¥å…·æœªå¯ç”¨"} for sq in subquestions]

        print(f"ðŸ” æ­£åœ¨è¿›è¡Œ Web Researchï¼Œå…± {len(subquestions)} ä¸ªå­é—®é¢˜...")
        aggregated = []
        for i, sq in enumerate(subquestions, 1):
            query = sq["subq"]
            print(f"  æœç´¢é—®é¢˜ {i}: {query}")
            try:
                results = search_tool.run(query)
                print(f"    æ‰¾åˆ° {len(results)} ä¸ªç»“æžœ")
                # æ‰“å°æœç´¢ç»“æžœæ‘˜è¦
                for j, result in enumerate(results[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æžœ
                    title = result.get('title', '(æ— æ ‡é¢˜)')
                    snippet = result.get('snippet', '')[:100] + '...' if len(result.get('snippet', '')) > 100 else result.get('snippet', '')
                    url = result.get('url', '')
                    print(f"      [{j}] {title}")
                    print(f"          {snippet}")
                    print(f"          URL: {url}")
                err = ""
            except Exception as e:
                results = []
                err = str(e)
                print(f"    æœç´¢å¤±è´¥: {err}")
            aggregated.append({
                "subq": query,
                "reason": sq.get("reason", ""),
                "results": results,
                "error": err
            })
        print("ðŸ” Web Research å®Œæˆ")
        return aggregated

    def _synthesize(self, query: str, search_data: List[Dict[str, Any]]) -> str:
        snippets_lines = []
        idx = 1
        for block in search_data:
            results = block.get("results", [])
            if not results:
                snippets_lines.append(f"[{idx}] (æ— ç»“æžœ) {block['subq']}")
                idx += 1
                continue
            for r in results:
                line = f"[{idx}] {r.get('title','(æ— æ ‡é¢˜)')} | {r.get('snippet','')} | {r.get('url','')}"
                snippets_lines.append(line)
                idx += 1

        synth_prompt = SYNTHESIS_PROMPT.format(
            query=query,
            snippets="\n".join(snippets_lines) if snippets_lines else "(æ— æœç´¢æ•°æ®)"
        )

        response = self.llm.invoke([self.system_message, HumanMessage(content=synth_prompt)])
        return response.content

    def ask(self, query: str) -> Dict[str, Any]:
        self.memory.add("user", query)
        plan = self._plan(query)
        search_data = self._search(plan)
        answer_markdown = self._synthesize(query, search_data)
        self.memory.add("assistant", answer_markdown)
        self.memory.maybe_compress()

        return {
            "plan": plan,
            "search_raw": search_data,
            "answer_markdown": answer_markdown
        }

    def critique(self, feedback: str) -> Dict[str, Any]:
        self.memory.add("user", feedback)
        critique_prompt = CRITIQUE_PROMPT.format(feedback=feedback)
        response = self.llm.invoke([self.system_message, HumanMessage(content=critique_prompt)])
        txt = response.content.strip()
        try:
            data = json.loads(txt)
        except Exception:
            data = {
                "need_new_search": False,
                "new_subquestions": [],
                "improved_answer": f"è§£æžåé¦ˆ JSON å¤±è´¥ï¼ŒåŽŸå§‹å†…å®¹ï¼š\n{txt}"
            }

        if data.get("need_new_search") and data.get("new_subquestions"):
            subqs = [{"subq": q, "reason": "ç”¨æˆ·è´¨ç–‘åŽæ–°å¢ž"} for q in data["new_subquestions"]]
            search_data = self._search(subqs)
            improved_full = self._synthesize(data["new_subquestions"][0], search_data)
            data["improved_answer"] += "\n\n## æ–°å¢žè¡¥å……æœç´¢ç»¼åˆ\n" + improved_full
        else:
            search_data = []

        self.memory.add("assistant", data["improved_answer"])
        self.memory.maybe_compress()

        return {
            "critique_result": data,
            "new_search_raw": search_data
        }

    def export_state(self) -> Dict[str, Any]:
        return {
            "compressed_context": self.memory.compressed_context,
            "messages": [
                {"role": m.role, "content": m.content}
                for m in self.memory.messages
            ]
        }

    def continue_dialog(self, user_message: str) -> str:
        self.memory.add("user", user_message)
        context_msgs = [self.system_message] + [
            HumanMessage(content=m.content) if m.role == "user" else AIMessage(content=m.content)
            for m in self.memory.messages
        ]
        response = self.llm.invoke(context_msgs)
        self.memory.add("assistant", response.content)
        self.memory.maybe_compress()
        return response.content